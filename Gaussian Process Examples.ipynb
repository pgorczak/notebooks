{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "def plot_samples(t, Zs, wrap, size=(1.5, 1.5)):\n",
    "    \"\"\" Plot a number of processes in a grid.\n",
    "    \n",
    "    Args:\n",
    "        t: index array of size n\n",
    "        Zs: mxn array of m process samples\n",
    "        wrap: start a new line after this number of plots\n",
    "        size: (width, height) of each plot\n",
    "    \"\"\"\n",
    "    n_samples = Zs.shape[0]\n",
    "    df = pd.DataFrame(dict(\n",
    "        sample=np.repeat(range(n_samples), len(t)),\n",
    "        t=np.tile(t, n_samples),\n",
    "        Z=Zs.ravel()))\n",
    "    w, h = size\n",
    "    grid = sns.FacetGrid(df, col='sample', hue='sample', col_wrap=wrap, size=h, aspect=w/h)\n",
    "    grid.map(plt.plot, 't', 'Z', marker=\"o\", ms=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process\n",
    "\n",
    "GPs are a nice tool for probabilistic regression.\n",
    "\n",
    "## Links\n",
    "- [mathematicalmonk on YouTube](https://www.youtube.com/watch?v=vU6AiEYED9E&list=PLD0F06AA0D2E8FFBA&index=150)\n",
    "- **TODO** Nando de Freitas on YouTube\n",
    "\n",
    "## Context and definitions\n",
    "A random process (aka stochastic process) is a collection of random variables (outputs). In a GP, $n$ elements from some index set $S$ are mapped to random variables $Z_t: t \\in S$, and $(Z_{t1}, ..., Z_{tn})$ form a multivariate Gaussian distribution.\n",
    "\n",
    "### Intuition\n",
    "Often, the index set $S$ contains points in time. Now all *outputs*—let's call them *states*—associated with a point in time $Z_t$ form a multivariate Gaussian distribution. This means that each *state at some time* of the GP is another *dimension* of the overall Gaussian. The key point of GP is, that an **entire family of $S$ to output mappings** is **described by the characteristics** of this overall distribution. It has some mean $\\mu$ and—more importantly—a covariance $C$. The **covariance links** an **output** $Z_i$ to **all other outputs** (in general both past and future).\n",
    "\n",
    "### Example: random lines\n",
    "Let's pick some random slope $m \\sim \\mathcal{N}(0, 1)$ (Gaussian with mean 0 and variance 1). Our index can be any real number, so $S = \\mathbb{R}$. If we set our outputs to be\n",
    "\n",
    "$$\n",
    "Z_t = mt\n",
    "$$\n",
    "\n",
    "we get an overall Gaussian (by the affine property)\n",
    "\n",
    "$$\n",
    "(Z_{t0}, Z_{t1}, ...)^T = (t_0, t_1, ...)^T \\mathcal{N}(0, 1)\n",
    "$$\n",
    "\n",
    "So this process is indeed a GP.\n",
    "\n",
    "Let's plot some samples from this process the straightforward way. Next up we'll see, how to express this GP through mean and covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_lines():\n",
    "    ms = np.random.randn(8)\n",
    "    t = np.linspace(0, 1, 5)\n",
    "    # Shortcut for computing m*t for each m and t\n",
    "    lines = np.outer(ms, t)\n",
    "    plot_samples(t, lines, 4)\n",
    "    \n",
    "plot_random_lines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean and covariance\n",
    "Remember the key point: *we can express a process like random lines by the $\\mu$ and $C$ of a Gaussian distribution*. In general, we let $\\mu$ be a function $\\mu: S \\to \\mathbb{R}$. The covariance matrix $C$ is defined by another function, the **kernel**, $k: S \\times S \\to \\mathbb{R}$, such that $C_{ij} = k(t_i, t_j)$. With this, we have defined our overall Gaussian. Note that the covariance of two outputs is defined by a function on their indices: $\\mathrm{Cov}(Z_i, Z_j) = k(t_i, t_j)$.\n",
    "\n",
    "### Random lines revisited\n",
    "We can re-formulate the random lines process. Its mean function is zero and its covariance function is $k(t_i, t_j) = t_i t_j$. Let's draw some samples from that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the warning if you get one; it's due to numerical\n",
    "# precision errors. You can modify the function to verify\n",
    "# that all eigenvalues of C are non-negative or very close\n",
    "# to zero.\n",
    "\n",
    "def plot_random_lines_2():\n",
    "    # Define input set\n",
    "    t = np.linspace(0, 1, 5)\n",
    "    # Calculate mu and C\n",
    "    mu = np.zeros(5)\n",
    "    C = np.array([[i * j for j in t] for i in t])\n",
    "    # Draw samples\n",
    "    lines = np.random.multivariate_normal(mu, C, 8)\n",
    "    plot_samples(t, lines, 4)\n",
    "    \n",
    "    \n",
    "plot_random_lines_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition\n",
    "Note that in the example above, we have defined a multivariate Gaussian, from which each sample is a line! It is shaped such that in each sample $(Z_1, Z_2, Z_3, Z_4, Z_5)^T$ the components form a straight line. For further examples, let's define a GP-sampling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp(mu, k, t, dimension=(2, 4), size=(1.5, 1.5)):\n",
    "    \"\"\"Plot samples of a Gaussian process.\n",
    "    \n",
    "    Args:\n",
    "        mu: mean function.\n",
    "        k: covariance function.\n",
    "        t: array of inputs.\n",
    "        dimension: tuple (h, w); make a grid of h x w samples/plots.\n",
    "        size: (width, height) of each plot\n",
    "    Returns\n",
    "        Mean and covariance matrix of the GP-distribution.\n",
    "    \"\"\"\n",
    "    mu_ = np.array([mu(i) for i in t])\n",
    "    C = np.array([[k(i, j) for j in t] for i in t])\n",
    "    h, w = dimension\n",
    "    samples = np.random.multivariate_normal(mu_, C, h * w)\n",
    "    plot_samples(t, samples, w, size)\n",
    "    return mu_, C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $\\mu$ defines your *mean* output or the expected output. The covariance $k$ defines the *shape over time* of the output. Let's see what happens if we give random lines a different mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_linear_variations():\n",
    "    t = np.arange(10)\n",
    "\n",
    "    k = lambda i, j: i * j\n",
    "    \n",
    "    mu_slope = lambda i: 3 * i\n",
    "    mu_zigzag = lambda i: 3 * (2 * (i%2) - 1)\n",
    "\n",
    "    plot_gp(mu_slope, k, t, (1, 6))\n",
    "    _, C = plot_gp(mu_zigzag, k, t, (1, 6))\n",
    "    \n",
    "    print('Covariance matrix:')\n",
    "    print(C)\n",
    "    \n",
    "plot_linear_variations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear how $\\mu$ shapes the possible outputs. In this case our choice of $k$ seems to create an *increasing offset over time* from the mean output. This is reflected in the growing *variance* of the main diagonal elements. The off-diagonal covariance elements impose *constraints* on the distribution such that only linear lines are possible.\n",
    "\n",
    "\n",
    "## More examples\n",
    "Here are some more examples of kernels and samples from resulting GPs.\n",
    "\n",
    "### Gaussian noise\n",
    "The simplest GP just has $C = \\sigma^2 I$. This means that there is no correlation between any two output points and any output has variance $\\sigma^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, C = plot_gp(lambda i: 0, lambda i, j: 1 if i == j else 0, np.arange(100), (2, 2), (6, 1.5))\n",
    "print('Covariance matrix:')\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brownian motion\n",
    "Brownian motion (aka Wiener process) is a GP with kernel $k(i, j) = min(i, j)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, C = plot_gp(lambda i: 0, lambda i, j: min(i, j), np.arange(100), (2, 2), (6, 1.5))\n",
    "print('Covariance matrix:')\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative exponential\n",
    "The negative exponential kernel $k(i,j) = \\exp(-|i - j|^2)$. Since it only depends on the distance between inputs, resulting GPs are called *stationary*. Since the absolute value of the distance is used (direction doesn't matter), they're also *isotropic*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, C = plot_gp(lambda i: 0, lambda i, j: np.exp(-(i - j)**2), np.linspace(0,10), (2, 2), (6, 1.5))\n",
    "print('Covariance matrix:')\n",
    "print(C)\n",
    "print('\\n\\n20th line of the covariance matrix:')\n",
    "plt.plot(C[19, :], marker='o', linestyle='')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
