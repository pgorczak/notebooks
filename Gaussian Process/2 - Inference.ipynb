{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Inference in Gaussian processes\n",
    "\n",
    "* [mathematicalmonk on GP inference](https://youtu.be/UH1d2mxwet8)\n",
    "\n",
    "We've seen that a GP is defined by a multivariate normal distribution over *outputs associated with some indices* or *states at some points in time*. Given a number of known states, we can get conditional distributions over one or more unobserved states. We can query the GP for certain indices/times.\n",
    "\n",
    "Let's separate $Z$ into a known part $Z_b$ and unknown part $Za$:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "    Z_a \\\\\n",
    "    Z_b\n",
    "\\end{pmatrix}\n",
    "\\sim\n",
    "\\mathcal{N}(\n",
    "    \\begin{pmatrix}\n",
    "        \\mu_a \\\\\n",
    "        \\mu_b\n",
    "    \\end{pmatrix}\n",
    ",\n",
    "    \\begin{pmatrix}\n",
    "        K_{aa} && K_{ab} \\\\\n",
    "        K_{ba} && K_{bb}\n",
    "    \\end{pmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "Given some concrete values $Z_b = z_b$, the conditional is another Gaussian:\n",
    "\n",
    "$$\n",
    "(Z_a | Z_b = z_b) \\sim \\mathcal{N}(m, D)\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "m &= \\mu_a + K_{ab} K_{bb}^{-1}(z_b - \\mu_b) \\\\\n",
    "D &= K_{aa} - K_{ab} K_{bb}^{-1} K_{ba}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "## Example: polynomials\n",
    "Let's transform our linear kernel into a polynomial kernel. If you want to know more about which transforms are allowed for kernels, there's a [video](https://youtu.be/Sc5hOS5HqdY). In this case, we choose a polynomial with non-negative coefficients. Let's look at some samples for\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu(i) &= 0 \\\\\n",
    "k(i, j) &= 0.1 (ij)^3 + 0.3 (ij)^2 + ij\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from utilities import plot_gp\n",
    "    \n",
    "_ = plot_gp(\n",
    "    mu=lambda i: 0,\n",
    "    k=lambda i, j: np.polynomial.polynomial.polyval(i * j, [0.1, 0.3, 1, 0]),\n",
    "    t=np.linspace(-5, 5, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing mean and variance\n",
    "We can use $K$ to get a variance for each output over the whole process (the entries on its diagonal). Let's visualize that together with some more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from utilities import covariance, mean\n",
    "\n",
    "mu = lambda i: 0\n",
    "k = lambda i, j: np.polynomial.polynomial.polyval(i * j, [0.1, 0.3, 1, 0])\n",
    "t = np.linspace(-5, 5, 50)\n",
    "    \n",
    "# These functions just evaluate mu and k given t\n",
    "mu_ = mean(mu, t)\n",
    "K = covariance(k, t, t)\n",
    "# Get the range of two standard deviations for each output\n",
    "two_sigma = 2 * np.sqrt(np.diag(K))\n",
    "    \n",
    "# Draw some samples\n",
    "samples = np.random.multivariate_normal(mu_, K, 30)\n",
    "\n",
    "# Plot\n",
    "sns.set_style('darkgrid')\n",
    "for s in samples:\n",
    "    plt.plot(t, s, alpha=0.3)\n",
    "plt.plot(t, mu_, label='μ', linewidth=3)\n",
    "plt.fill_between(t, mu_ - two_sigma, mu_ + two_sigma, label='2σ', alpha=0.4)\n",
    "plt.xlim(-5, 5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing observations\n",
    "Let's say we've observed outputs $(-4, 10)$ and $(1, 3)$. Keeping the same $t$ as in the previous plot, we can calculate and visualize the conditional distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ta = t\n",
    "mua = mu_\n",
    "Kaa = K\n",
    "\n",
    "tb = np.array([-4, 1])\n",
    "Zb = np.array([10, 3])\n",
    "mub = mean(mu, tb)\n",
    "Kbb = covariance(k, tb, tb)\n",
    "\n",
    "Kab = covariance(k, ta, tb)\n",
    "Kbb_inv = np.linalg.inv(Kbb)\n",
    "\n",
    "m = mua + Kab.dot(Kbb_inv).dot(Zb - mub)\n",
    "# NOTE that we use Kab^T instead of implicitly computing Kba.\n",
    "# They are equal because covariance matrices are always symmetric.\n",
    "D = Kaa - Kab.dot(Kbb_inv).dot(Kab.T)\n",
    "two_sigma_posterior = 2 * np.sqrt(np.diag(D))\n",
    "\n",
    "# Plot\n",
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(ta, m, label='μ')\n",
    "plt.fill_between(ta, m - two_sigma_posterior, m + two_sigma_posterior, label='2σ', alpha=0.4)\n",
    "plt.scatter(tb, Zb, label='observations')\n",
    "plt.xlim(-5, 5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition\n",
    "In the plot above, you see how observations can be used to get a **posterior** GP. The posterior can only generate outputs that match the observations. Any sample from the posterior passes through the observed points (i.e. if the posterior index includes the observed index, the outputs match the observation at that index). Two GP characteristics are especially notable:\n",
    "\n",
    "* Given a small set of observations and GP-hyperparameters ($\\mu$ and $k$), we can compute a **posterior process for any indexes** we desire. Usually, we even assume that indexes come from $\\mathbb{R}^n$. For the plot, we sample a set of real numbers in $[-5, 5]$ (selected by `np.linspace`). We could also just get a posterior distribution of the output at the single index $1000$.\n",
    "* In addition to the mean or *expected process*, the posterior includes a measure of variance. You can see in the plot how the **variance is squeezed at the observed points** and how it expands when we're far away from observations.\n",
    "\n",
    "As a final visualization, here are full posterior distributions for outputs at $2$ and $4$. You'll see that we're pretty sure the output is around $7.5$ in the first case, but not so sure about the second case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "from utilities import posterior\n",
    "\n",
    "m_1, D_1 = posterior(mu, k, [2], tb, Zb)\n",
    "m_2, D_2 = posterior(mu, k, [4], tb, Zb)\n",
    "\n",
    "# Extract mean and standard deviation\n",
    "# (item() extracts the scalar from an ndarray with only one element)\n",
    "mean_1 = m_1.item()\n",
    "std_dev_1 = np.sqrt(D_1.item())\n",
    "mean_2 = m_2.item()\n",
    "std_dev_2 = np.sqrt(D_2.item())\n",
    "\n",
    "# Plot pdfs\n",
    "Z = np.linspace(0, 40, 1000)\n",
    "pdf_1 = scipy.stats.norm.pdf(Z, mean_1, std_dev_1)\n",
    "pdf_2 = scipy.stats.norm.pdf(Z, mean_2, std_dev_2)\n",
    "sns.set_style('darkgrid')\n",
    "plt.plot(Z, pdf_1, label='PDF at t=2')\n",
    "plt.plot(Z, pdf_2, label='PDF at t=4')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Noisy observations\n",
    "We can add some uncertainty priors to our observations. The key step to this is, that we look at the true outputs $Z$ via some noisy proxy $Y = Z + \\varepsilon$, introducing errors $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2I)$. Our additional new prior is the variance/uncertainty of observing outputs. The new GP is simply\n",
    "\n",
    "$$\n",
    "Y \\sim \\mathcal{N}(\\mu, K + \\sigma^2I)\n",
    "$$\n",
    "\n",
    "And the conditional step changes to\n",
    "\n",
    "$$\n",
    "(Y_a | Y_b = y_b) \\sim \\mathcal{N}(m, D)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "m &= \\mu_a + K_{ab} (K_{bb} + \\sigma^2 I)^{-1}(z_b - \\mu_b) \\\\\n",
    "D &= (K_{aa} + \\sigma^2 I) - K_{ab} (K_{bb} + \\sigma^2 I)^{-1} K_{ba}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If you're familiar with numerical optimization, you might see how the uncertainty $\\sigma^2$ acts like a regularizing term.\n",
    "\n",
    "To conclude, here are some examples of our posterior with different uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from utilities import noisy_posterior\n",
    "\n",
    "def get_noisy_posterior(sigma):\n",
    "    m, D = noisy_posterior(mu, k, sigma, ta, tb, Zb)\n",
    "    two_sigma = 2 * np.sqrt(np.diag(D))\n",
    "    return np.column_stack((ta, m, two_sigma, np.repeat(sigma, len(ta))))\n",
    "\n",
    "def plot_noisy_posterior(t, m, two_sigma, label, color):\n",
    "    plt.plot(t, m, color=color)\n",
    "    plt.fill_between(t, m - two_sigma, m + two_sigma, color=color, alpha=0.4)\n",
    "    plt.scatter(tb, Zb, color=color)\n",
    "\n",
    "posteriors = np.concatenate(\n",
    "    [get_noisy_posterior(s) for s in (0.1, 0.2, 1, 5)],\n",
    "    axis=0)\n",
    "\n",
    "grid = sns.FacetGrid(\n",
    "    pd.DataFrame(posteriors, columns=['t', 'm', 'two_sigma', 'sigma']),\n",
    "    col='sigma',\n",
    "    hue='sigma',\n",
    "    col_wrap=2,\n",
    "    size=6,\n",
    "    aspect=1)\n",
    "\n",
    "sns.set_style('ticks')\n",
    "grid.map(plot_noisy_posterior, 't', 'm', 'two_sigma')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
